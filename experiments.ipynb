{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcc/Documents/msc/courses/c550/nlp/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI, ChatCohere, ChatAnyscale\n",
    "\n",
    "from main import get_query_expansion_dataset, run_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not initialize Cohere chatbot. Please store a valid API key in a .env file.\n"
     ]
    }
   ],
   "source": [
    "# set up LLMs, assumes that API keys are stored in .env file\n",
    "load_dotenv(override=True)\n",
    "chats = {}\n",
    "\n",
    "# OpenAI\n",
    "try: \n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    chat_openai = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "    chats[\"OpenAI\"] = chat_openai\n",
    "except:\n",
    "    print(\"Could not initialize OpenAI chatbot. Please store a valid API key in a .env file.\")\n",
    "\n",
    "# Cohere\n",
    "try:\n",
    "    cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "    chat_cohere = ChatCohere(cohere_api_key=cohere_api_key)\n",
    "    chats[\"Cohere\"] = chat_cohere\n",
    "except:\n",
    "    print(\"Could not initialize Cohere chatbot. Please store a valid API key in a .env file.\")\n",
    "\n",
    "# Llama 2 (Anyscale)\n",
    "try: \n",
    "    anyscale_api_key = os.getenv(\"ANYSCALE_API_KEY\")\n",
    "    chat_llama = ChatAnyscale(model_name=\"meta-llama/Llama-2-7b-chat-hf\", anyscale_api_key=anyscale_api_key)\n",
    "    chats[\"Llama 2\"] = chat_llama\n",
    "except:\n",
    "    print(\"Could not initialize Anyscale chatbot. Please store a valid API key in a .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments Set 1: All prompts, english language, all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Miracl dataset (english version)\n",
    "dataset = load_dataset(\"Cohere/miracl-en-queries-22-12\", split=\"dev\")\n",
    "dataset = dataset.to_pandas().to_dict(orient='records')\n",
    "\n",
    "# set up searcher\n",
    "searcher = LuceneSearcher.from_prebuilt_index('miracl-v1.0-en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 799/799 [01:37<00:00,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Recall@100: 0.8190\n",
      "BM25 nDCG@10: 0.3506\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall_baseline, ndcg_baseline = run_search(searcher, dataset)\n",
    "\n",
    "print(f'BM25 Recall@100: {recall_baseline:.4f}')\n",
    "print(f'BM25 nDCG@10: {ndcg_baseline:.4f}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Answer prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching:   0%|          | 0/799 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 799/799 [06:56<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Recall@100: 0.8984\n",
      "OpenAI nDCG@10: 0.5122\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 799/799 [55:57<00:00,  4.20s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 Recall@100: 0.8424\n",
      "Llama 2 nDCG@10: 0.4219\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for chat_name, chat in chats.items():\n",
    "    expanded_dataset = get_query_expansion_dataset(dataset, chat, chat_name, lang='en', prompt='answer')\n",
    "\n",
    "    recall, ndcg = run_search(searcher, expanded_dataset)\n",
    "    print(f'{chat_name} Recall@100: {recall:.4f}') \n",
    "    print(f'{chat_name} nDCG@10: {ndcg:.4f}')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching:   0%|          | 0/799 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 799/799 [01:27<00:00,  9.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Recall@100: 0.8433\n",
      "OpenAI nDCG@10: 0.3745\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding queries: 100%|██████████| 799/799 [36:00<00:00,  2.70s/it]\n",
      "Searching: 100%|██████████| 799/799 [19:37<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 Recall@100: 0.8668\n",
      "Llama 2 nDCG@10: 0.4314\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for chat_name, chat in chats.items():\n",
    "    expanded_dataset = get_query_expansion_dataset(dataset, chat, chat_name, lang='en', prompt='keywords')\n",
    "\n",
    "    recall, ndcg = run_search(searcher, expanded_dataset)\n",
    "    print(f'{chat_name} Recall@100: {recall:.4f}') \n",
    "    print(f'{chat_name} nDCG@10: {ndcg:.4f}')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Keywords with pseudo-relevant feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching:   1%|          | 5/799 [00:01<03:27,  3.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 799/799 [02:41<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Recall@100: 0.8337\n",
      "OpenAI nDCG@10: 0.3722\n",
      "\n",
      "\n",
      "Getting prf documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 799/799 [00:26<00:00, 29.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding queries: 100%|██████████| 799/799 [45:29<00:00,  3.42s/it]\n",
      "Searching: 100%|██████████| 799/799 [29:49<00:00,  2.24s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 Recall@100: 0.8035\n",
      "Llama 2 nDCG@10: 0.3635\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for chat_name, chat in chats.items():\n",
    "    expanded_dataset = get_query_expansion_dataset(dataset, chat, chat_name, lang='en', prompt='keywords-prf')\n",
    "\n",
    "    recall, ndcg = run_search(searcher, expanded_dataset)\n",
    "    print(f'{chat_name} Recall@100: {recall:.4f}') \n",
    "    print(f'{chat_name} nDCG@10: {ndcg:.4f}')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Chain of thought prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching:   0%|          | 0/799 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 799/799 [24:42<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Recall@100: 0.8866\n",
      "OpenAI nDCG@10: 0.4877\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 799/799 [1:03:32<00:00,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 Recall@100: 0.8334\n",
      "Llama 2 nDCG@10: 0.4340\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for chat_name, chat in chats.items():\n",
    "    expanded_dataset = get_query_expansion_dataset(dataset, chat, chat_name, lang='en', prompt='chain-of-thought')\n",
    "\n",
    "    recall, ndcg = run_search(searcher, expanded_dataset)\n",
    "    print(f'{chat_name} Recall@100: {recall:.4f}') \n",
    "    print(f'{chat_name} nDCG@10: {ndcg:.4f}')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Chain of thought prompting with psuedo-relevant feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching:   0%|          | 0/799 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 799/799 [09:02<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Recall@100: 0.8486\n",
      "OpenAI nDCG@10: 0.4343\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 799/799 [22:02<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 Recall@100: 0.8285\n",
      "Llama 2 nDCG@10: 0.4265\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for chat_name, chat in chats.items():\n",
    "    expanded_dataset = get_query_expansion_dataset(dataset, chat, chat_name, lang='en', prompt='chain-of-thought-prf')\n",
    "\n",
    "    recall, ndcg = run_search(searcher, expanded_dataset)\n",
    "    print(f'{chat_name} Recall@100: {recall:.4f}') \n",
    "    print(f'{chat_name} nDCG@10: {ndcg:.4f}')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Chain of thought prompting with short answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 799/799 [08:47<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Recall@100: 0.9027\n",
      "OpenAI nDCG@10: 0.5154\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding queries: 100%|██████████| 799/799 [27:47<00:00,  2.09s/it]\n",
      "Searching: 100%|██████████| 799/799 [10:15<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 Recall@100: 0.8890\n",
      "Llama 2 nDCG@10: 0.4775\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for chat_name, chat in chats.items():\n",
    "    expanded_dataset = get_query_expansion_dataset(dataset, chat, chat_name, lang='en', prompt='chain-of-thought-short')\n",
    "\n",
    "    recall, ndcg = run_search(searcher, expanded_dataset)\n",
    "    print(f'{chat_name} Recall@100: {recall:.4f}') \n",
    "    print(f'{chat_name} nDCG@10: {ndcg:.4f}')\n",
    "    print(\"\\n\")\n",
    "\n",
    "# openai time: 2:04:00 h\n",
    "# llama2 time: 0:27:47 h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments Set 2: Chain-of-thought prompt, various languages, all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Miracl dataset (english version)\n",
    "dataset = load_dataset(\"Cohere/miracl-fr-queries-22-12\", split=\"dev\")\n",
    "dataset = dataset.to_pandas().to_dict(orient='records')\n",
    "\n",
    "# set up searcher\n",
    "searcher = LuceneSearcher.from_prebuilt_index('miracl-v1.0-fr')\n",
    "searcher.set_language('fr')\n",
    "\n",
    "# exclude cohere model as it does not support french\n",
    "chats_fr = {k: v for k, v in chats.items() if k != \"Cohere\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 343/343 [00:27<00:00, 12.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Recall@100: 0.6528\n",
      "BM25 nDCG@10: 0.1832\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall_baseline, ndcg_baseline = run_search(searcher, dataset)\n",
    "\n",
    "print(f'BM25 Recall@100: {recall_baseline:.4f}')\n",
    "print(f'BM25 nDCG@10: {ndcg_baseline:.4f}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding queries:   0%|          | 0/343 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding queries: 100%|██████████| 343/343 [18:35<00:00,  3.25s/it]\n",
      "Searching: 100%|██████████| 343/343 [00:45<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Recall@100: 0.7643\n",
      "OpenAI nDCG@10: 0.3016\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding queries: 100%|██████████| 343/343 [16:49<00:00,  2.94s/it]\n",
      "Searching: 100%|██████████| 343/343 [01:12<00:00,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 Recall@100: 0.6089\n",
      "Llama 2 nDCG@10: 0.2549\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for chat_name, chat in chats_fr.items():\n",
    "    expanded_dataset = get_query_expansion_dataset(dataset, chat, chat_name, lang='fr', prompt='chain-of-thought-fr')\n",
    "\n",
    "    recall, ndcg = run_search(searcher, expanded_dataset)\n",
    "    print(f'{chat_name} Recall@100: {recall:.4f}') \n",
    "    print(f'{chat_name} nDCG@10: {ndcg:.4f}')\n",
    "    print(\"\\n\")\n",
    "\n",
    "# openai time: 18:35 min\n",
    "# llama 2 time: 16:49 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Miracl dataset (english version)\n",
    "dataset = load_dataset(\"Cohere/miracl-de-queries-22-12\", split=\"dev\")\n",
    "dataset = dataset.to_pandas().to_dict(orient='records')\n",
    "\n",
    "# set up searcher\n",
    "searcher = LuceneSearcher.from_prebuilt_index('miracl-v1.0-de')\n",
    "searcher.set_language('de')\n",
    "\n",
    "# exclude cohere model as it does not support french\n",
    "chats_de = {k: v for k, v in chats.items() if k != \"Cohere\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching:   0%|          | 0/305 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 305/305 [00:27<00:00, 11.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Recall@100: 0.5724\n",
      "BM25 nDCG@10: 0.2262\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall_baseline, ndcg_baseline = run_search(searcher, dataset)\n",
    "\n",
    "print(f'BM25 Recall@100: {recall_baseline:.4f}')\n",
    "print(f'BM25 nDCG@10: {ndcg_baseline:.4f}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding queries:   0%|          | 0/305 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding queries: 100%|██████████| 305/305 [15:05<00:00,  2.97s/it]\n",
      "Searching: 100%|██████████| 305/305 [00:39<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Recall@100: 0.7225\n",
      "OpenAI nDCG@10: 0.3442\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding queries: 100%|██████████| 305/305 [11:01<00:00,  2.17s/it]\n",
      "Searching: 100%|██████████| 305/305 [00:53<00:00,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 Recall@100: 0.6099\n",
      "Llama 2 nDCG@10: 0.2864\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for chat_name, chat in chats_de.items():\n",
    "    expanded_dataset = get_query_expansion_dataset(dataset, chat, chat_name, lang='de', prompt='chain-of-thought-de')\n",
    "\n",
    "    recall, ndcg = run_search(searcher, expanded_dataset)\n",
    "    print(f'{chat_name} Recall@100: {recall:.4f}') \n",
    "    print(f'{chat_name} nDCG@10: {ndcg:.4f}')\n",
    "    print(\"\\n\")\n",
    "\n",
    "# openai time: 15:05 min\n",
    "# llama 2 time: 11:01 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Miracl dataset (english version)\n",
    "dataset = load_dataset(\"Cohere/miracl-zh-queries-22-12\", split=\"dev\")\n",
    "dataset = dataset.to_pandas().to_dict(orient='records')\n",
    "\n",
    "# set up searcher\n",
    "searcher = LuceneSearcher.from_prebuilt_index('miracl-v1.0-zh')\n",
    "searcher.set_language('zh')\n",
    "\n",
    "# exclude cohere model as it does not support french\n",
    "chats_zh = {k: v for k, v in chats.items() if k != \"Cohere\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 393/393 [00:18<00:00, 21.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Recall@100: 0.5599\n",
      "BM25 nDCG@10: 0.1801\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall_baseline, ndcg_baseline = run_search(searcher, dataset)\n",
    "\n",
    "print(f'BM25 Recall@100: {recall_baseline:.4f}')\n",
    "print(f'BM25 nDCG@10: {ndcg_baseline:.4f}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding queries: 100%|██████████| 393/393 [27:57<00:00,  4.27s/it]\n",
      "Searching: 100%|██████████| 393/393 [00:46<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Recall@100: 0.6929\n",
      "OpenAI nDCG@10: 0.2872\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding queries: 100%|██████████| 393/393 [26:40<00:00,  4.07s/it]  \n",
      "Searching: 100%|██████████| 393/393 [00:22<00:00, 17.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 Recall@100: 0.5007\n",
      "Llama 2 nDCG@10: 0.1451\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for chat_name, chat in chats_zh.items():\n",
    "    expanded_dataset = get_query_expansion_dataset(dataset, chat, chat_name, lang='zh', prompt='chain-of-thought-zh')\n",
    "\n",
    "    recall, ndcg = run_search(searcher, expanded_dataset)\n",
    "    print(f'{chat_name} Recall@100: {recall:.4f}') \n",
    "    print(f'{chat_name} nDCG@10: {ndcg:.4f}')\n",
    "    print(\"\\n\")\n",
    "\n",
    "# openai time: 27:57 min\n",
    "# llama 2 time: 26:40 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Miracl dataset (english version)\n",
    "dataset = load_dataset(\"Cohere/miracl-es-queries-22-12\", split=\"dev\")\n",
    "dataset = dataset.to_pandas().to_dict(orient='records')\n",
    "\n",
    "# set up searcher\n",
    "searcher = LuceneSearcher.from_prebuilt_index('miracl-v1.0-es')\n",
    "searcher.set_language('es')\n",
    "\n",
    "# exclude cohere model as it does not support french\n",
    "chats_es = {k: v for k, v in chats.items() if k != \"Cohere\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 648/648 [00:20<00:00, 31.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Recall@100: 0.7018\n",
      "BM25 nDCG@10: 0.3193\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall_baseline, ndcg_baseline = run_search(searcher, dataset)\n",
    "\n",
    "print(f'BM25 Recall@100: {recall_baseline:.4f}')\n",
    "print(f'BM25 nDCG@10: {ndcg_baseline:.4f}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 648/648 [00:42<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Recall@100: 0.7770\n",
      "OpenAI nDCG@10: 0.4034\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 648/648 [01:10<00:00,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 Recall@100: 0.7409\n",
      "Llama 2 nDCG@10: 0.3646\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for chat_name, chat in chats_es.items():\n",
    "    expanded_dataset = get_query_expansion_dataset(dataset, chat, chat_name, lang='es', prompt='chain-of-thought-es')\n",
    "\n",
    "    recall, ndcg = run_search(searcher, expanded_dataset)\n",
    "    print(f'{chat_name} Recall@100: {recall:.4f}') \n",
    "    print(f'{chat_name} nDCG@10: {ndcg:.4f}')\n",
    "    print(\"\\n\")\n",
    "\n",
    "# openai time: 27:57 min\n",
    "# llama 2 time: 26:40 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
